{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPwm7GcYoau3JBlxhlGOe1u",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pooja-Informatics/symmetrical-orchid/blob/main/speech_enhancement.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wave U Net Architecture For Speech Enhancement"
      ],
      "metadata": {
        "id": "NAsjvfiLE64G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Install modules and libraries**"
      ],
      "metadata": {
        "id": "Hjvw7x-Quz9S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RWrlPOgRuj5i"
      },
      "outputs": [],
      "source": [
        "!pip install librosa tensorflow keras numpy matplotlib pandas\n",
        "!pip install soundfile\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import**"
      ],
      "metadata": {
        "id": "YNJxnoEGGje5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import soundfile as sf\n",
        "import os\n"
      ],
      "metadata": {
        "id": "XQc-PwHxGVeM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dataset**"
      ],
      "metadata": {
        "id": "7v3ItiJbGsbN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "filePath ='/content/drive/MyDrive/Data_training/train_data/clean_trainset_wav'\n",
        "filePath ='/content/drive/MyDrive/Data_training/train_data/noisy_trainset_wav'"
      ],
      "metadata": {
        "id": "LDC28HSHG3YU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3e6d6b9-1626-4da6-fb03-2993b6bfc4f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**function to convert a folder of audio files into list of arrays of audio files**"
      ],
      "metadata": {
        "id": "a_gg8YV3Qjy2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import librosa\n",
        "#The librosa library to convert an audio folder containing audio files into a list of arrays:\n",
        "\n",
        "def convert_audio_folder_to_list(folder_path):\n",
        "    audio_list = []\n",
        "\n",
        "    for file_name in os.listdir(folder_path):\n",
        "        file_path = os.path.join(folder_path, file_name)\n",
        "\n",
        "        # Load audio file using librosa\n",
        "        audio_data, _ = librosa.load(file_path)\n",
        "\n",
        "        # Append audio data to the list\n",
        "        audio_list.append(audio_data)\n",
        "\n",
        "    return audio_list\n",
        "\n",
        "# Specify the path to the audio folder\n",
        "folder_path = \"/content/drive/MyDrive/Data_training/train_data/noisy_trainset_wav\"\n",
        "\n",
        "# Convert the audio folder to a list of arrays\n",
        "audio_list = convert_audio_folder_to_list(folder_path)\n",
        "\n",
        "# Print the list of audio arrays\n",
        "for audio_data in audio_list:\n",
        "    print(audio_data)\n",
        "\n",
        "   # The outputs contains representations of multiple 1-dimensional NumPy arrays.\n",
        "   #Each array consists of a sequence of float values.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3S6xbZIdnPrI",
        "outputId": "1b0a4749-f37f-4834-e804-2db4d008d515"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.01245543 0.02198051 0.02311831 ... 0.09252393 0.11061627 0.04900526]\n",
            "[-0.00696815 -0.01127945 -0.00957985 ...  0.00731695  0.00189796\n",
            "  0.        ]\n",
            "[-0.00474634 -0.00490906 -0.00352744 ...  0.00466625  0.00630934\n",
            "  0.00461349]\n",
            "[-0.00248011 -0.00380439 -0.00414199 ...  0.00536412  0.00697804\n",
            "  0.00536193]\n",
            "[-0.05526794 -0.05634881 -0.08667254 ...  0.00172743  0.00282566\n",
            "  0.00107446]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Generators**"
      ],
      "metadata": {
        "id": "zWGIjar5Y7GF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import soundfile as sf\n",
        "\n",
        "audio_folder = 'audio_files_folder'\n",
        "annotations_folder = 'annotations_folder'\n",
        "\n",
        "def audio_generator(files, batch_size=32):\n",
        "    while True:\n",
        "        # Extract a random batch\n",
        "        batch = np.random.choice(files, size=batch_size)\n",
        "\n",
        "        # Variables for collecting batches of inputs and outputs\n",
        "        batch_x = []\n",
        "        batch_y = []\n",
        "\n",
        "        for f in batch:\n",
        "            # Load the audio file\n",
        "            audio_path = os.path.join(audio_folder, f)\n",
        "            audio, _ = sf.read(audio_path)\n",
        "\n",
        "            # Load the corresponding annotation\n",
        "            annotation_path = os.path.join(annotations_folder, f[:-4] + '.txt')\n",
        "            annotation = np.loadtxt(annotation_path)\n",
        "\n",
        "            # Preprocess the audio and annotation\n",
        "            # Add your preprocessing steps here\n",
        "            processed_audio = preprocess_audio(audio)\n",
        "            processed_annotation = preprocess_annotation(annotation)\n",
        "\n",
        "            batch_x.append(processed_audio)\n",
        "            batch_y.append(processed_annotation)\n",
        "\n",
        "        # Preprocess a batch of audio and annotations\n",
        "        batch_x = np.array(batch_x)\n",
        "        batch_y = np.array(batch_y)\n",
        "\n",
        "        yield batch_x, batch_y\n"
      ],
      "metadata": {
        "id": "KM-LAp6uZDz3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**train and test generator**\n"
      ],
      "metadata": {
        "id": "Qpiiwnf4g5Ki"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "#split into training and testing\n",
        "train_files = all_files[0:split]\n",
        "test_files  = all_files[split:]\n",
        "train_generator = audio_generator(train_files, batch_size = batch_size)\n",
        "test_generator  = audio_generator(test_files, batch_size = batch_size)"
      ],
      "metadata": {
        "id": "Ggcvn3t3g_9K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mean**"
      ],
      "metadata": {
        "id": "S7I9dv9ZYHTK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow.keras.backend as K\n",
        "\n",
        "def mean_audio(y_true, y_pred):\n",
        "    tmp = np.mean(np.abs(y_true-y_pred))\n",
        "    return tmp\n"
      ],
      "metadata": {
        "id": "NNU5yH5ZS84b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model**"
      ],
      "metadata": {
        "id": "DqpVF7gZG4yF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "def unet(sz=(256, )):\n",
        "    x = Input(sz)\n",
        "    inputs = x\n",
        "\n",
        "    # Downsampling\n",
        "    f = 8\n",
        "    layers = []\n",
        "\n",
        "    for i in range(0, 6):\n",
        "        x = Conv2D(f, 3, activation='relu', padding='same')(x)\n",
        "        x = Conv2D(f, 3, activation='relu', padding='same')(x)\n",
        "        layers.append(x)\n",
        "        x = MaxPooling2D()(x)\n",
        "        f = f * 2\n",
        "    ff2 = 64\n",
        "\n",
        "    # Bottleneck\n",
        "    j = len(layers) - 1\n",
        "    x = Conv2D(f, 3, activation='relu', padding='same')(x)\n",
        "    x = Conv2D(f, 3, activation='relu', padding='same')(x)\n",
        "    x = Conv2DTranspose(ff2, 2, strides=(2, 2), padding='same')(x)\n",
        "    x = Concatenate(axis=3)([x, layers[j]])\n",
        "    j = j - 1\n",
        "\n",
        "    # Upsampling\n",
        "    for i in range(0, 5):\n",
        "        ff2 = ff2 // 2\n",
        "        f = f // 2\n",
        "        x = Conv2D(f, 3, activation='relu', padding='same')(x)\n",
        "        x = Conv2D(f, 3, activation='relu', padding='same')(x)\n",
        "        x = Conv2DTranspose(ff2, 2, strides=(2, 2), padding='same')(x)\n",
        "        x = Concatenate(axis=3)([x, layers[j]])\n",
        "        j = j - 1\n",
        "\n",
        "    # Reconstruction\n",
        "    x = Conv2D(f, 3, activation='relu', padding='same')(x)\n",
        "    x = Conv2D(f, 3, activation='relu', padding='same')(x)\n",
        "    outputs = Conv2D(1, 1, activation='sigmoid')(x)\n",
        "\n",
        "    # Model creation\n",
        "    model = Model(inputs=[inputs], outputs=[outputs])\n",
        "    model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=[temp])\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "8_snElkzG_l-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = unet(sz=(256, 256, 3))\n"
      ],
      "metadata": {
        "id": "ncOubNSCScG2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Call back**"
      ],
      "metadata": {
        "id": "dWf3gOyrUI8g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_callbacks():\n",
        "    checkpointer = ModelCheckpoint(filepath='wave_unet.h5', verbose=0, save_best_only=True, save_weights_only=True)\n",
        "    callbacks = [checkpointer, PlotLearning()]\n",
        "    return callbacks\n"
      ],
      "metadata": {
        "id": "UkTJI38AUO9Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training**"
      ],
      "metadata": {
        "id": "QAYoqGSPHALU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_steps = len(train_files) // batch_size\n",
        "test_steps = len(test_files) // batch_size\n",
        "\n",
        "model.fit(train_generator,\n",
        "          epochs=30,\n",
        "          steps_per_epoch=train_steps,\n",
        "          validation_data=test_generator,\n",
        "          validation_steps=test_steps,\n",
        "          callbacks=build_callbacks(),\n",
        "          verbose=0)\n"
      ],
      "metadata": {
        "id": "QllYFJ_jHH4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Testing**"
      ],
      "metadata": {
        "id": "r6lkbWYmHKyz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "filePath ='/content/drive/MyDrive/Data_training/train_data/clean_testset_wav'\n",
        "filePath ='/content/drive/MyDrive/Data_training/train_data/noisy_testset_wav'"
      ],
      "metadata": {
        "id": "1AT3nUvGHQ68",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53783fe0-9756-40cc-c175-54305d1d08b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    }
  ]
}